# 位置编码 (Position Encoding)

## 概述

Transformer模型本身没有循环结构（如RNN），因此无法感知序列中元素的位置信息。位置编码的作用就是为模型注入位置信息。

## 为什么需要位置编码？

**问题**：
- Self-Attention是置换不变的（permutation invariant）
- 如果交换输入序列的顺序，输出结果保持不变
- 但在NLP任务中，词序非常重要："我喜欢你" vs "你喜欢我"

**解决方案**：
在输入embedding中加入位置信息

## 位置编码方法

### 1. 正弦/余弦位置编码 (Sinusoidal Position Encoding)

这是原始Transformer论文中使用的方法。

**公式**：
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**参数说明**：
- `pos`: 位置索引（0, 1, 2, ...）
- `i`: 维度索引（0到d_model/2）
- `d_model`: 模型的维度（通常512）

**优点**：
- 不需要训练，节省参数
- 可以外推到比训练序列更长的序列
- 不同位置的相对距离可以被线性表示

**特性**：
- 每个维度对应不同频率的波
- 低频波捕捉长距离依赖
- 高频波捕捉短距离依赖

### 2. 可学习的位置编码 (Learned Position Encoding)

将位置编码作为可训练参数。

**实现**：
```python
self.position_embedding = nn.Embedding(max_seq_length, d_model)
```

**优点**：
- 可以学习到任务特定的位置模式
- 在某些任务上效果更好

**缺点**：
- 增加参数量
- 无法外推到更长序列
- 需要预设最大序列长度

### 3. 相对位置编码

不编码绝对位置，而是编码相对位置关系。

**优点**：
- 更好的泛化能力
- 对序列长度更鲁棒

## 位置编码的使用

**加法方式**（最常用）：
```
Input = Token_Embedding + Position_Encoding
```

**为什么用加法？**
- 保持维度一致
- 允许模型学习如何利用位置信息
- 简单有效

## 代码文件说明

### `positional_encoding.py`
实现不同类型的位置编码：
- 正弦/余弦位置编码
- 可学习位置编码
- 位置编码的可视化

### `test_encoding.py`
测试和对比：
- 验证位置编码的正确性
- 可视化位置编码模式
- 对比不同方法的效果

## 可视化理解

位置编码的可视化通常呈现以下模式：
- 横轴：位置
- 纵轴：维度
- 颜色：编码值

你会看到：
- 每个维度有不同的波动频率
- 形成独特的"条纹"模式
- 相似位置有相似的编码模式

## 关键要点

1. **位置编码是必需的**：弥补Transformer缺乏位置信息的不足
2. **正弦编码的数学美**：通过三角函数的周期性编码位置
3. **可学习 vs 固定**：是accuracy vs generalization的权衡
4. **加法融合**：最简单但有效的融合方式

## 实验建议

1. 实现正弦位置编码，观察其波动模式
2. 实现可学习位置编码，对比训练效果
3. 可视化位置编码矩阵，理解不同维度的作用
4. 测试在不同序列长度上的泛化能力

## 下一步

学习完位置编码后，继续学习 [Transformer模块](../03_transformer_blocks/)，了解如何组合注意力机制和其他组件构建完整的编码器和解码器。
